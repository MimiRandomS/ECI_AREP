{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad982ac",
   "metadata": {},
   "source": [
    "# Guía — Introducción práctica a Hugging Face (HF)\n",
    "Curso: **IA en el Aula — Nivel Avanzado**  \n",
    "Autor: **Luis Daniel Benavides Navarro**  \n",
    "Fecha: **Octubre 2025**\n",
    "\n",
    "Esta guía introduce el ecosistema de **Hugging Face**: conceptos, librerías principales, opciones de ejecución (local vs remoto) y ejemplos de código en Python. Incluye buenas prácticas, notas de licencias y consideraciones éticas en educación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b28a60",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Hugging Face?\n",
    "- **Hugging Face (HF)** es un ecosistema abierto para **modelos de aprendizaje automático**, datasets, espacios de demostración y herramientas.\n",
    "- Componentes clave:\n",
    "  - **Hugging Face Hub**: repositorio colaborativo de modelos, datasets y Spaces.\n",
    "  - **Librerías** (desarrolladas por **Hugging Face y su comunidad**):\n",
    "    - **transformers**: modelos de NLP/visión/audio (state-of-the-art) y utilidades de inferencia/entrenamiento.\n",
    "    - **datasets**: carga/limpieza/streaming de conjuntos de datos.\n",
    "    - **tokenizers**: tokenización eficiente (Rust/Python) para modelos modernos.\n",
    "    - **accelerate**, **peft**, **trl**: entrenamiento eficiente, fine-tuning ligero (LoRA), RLHF, etc.\n",
    "  - **Spaces**: apps (Gradio/Streamlit) desplegadas en la nube de HF.\n",
    "\n",
    "**¿Quién desarrolla estas librerías?**  \n",
    "Principalmente el **equipo de ingeniería de Hugging Face** junto con una **amplia comunidad open source** (universidades, empresas y desarrolladores independientes) que contribuyen vía pull requests, issues y discusiones. Los repositorios son públicos (licencias abiertas) y cuentan con mantenedores oficiales de HF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec75e73",
   "metadata": {},
   "source": [
    "## 2. Instalación rápida\n",
    "Ejecuta esta celda para instalar los paquetes base. En entornos gestionados, puedes omitir instalación si ya existen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9874245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\geronimo.martinez-n\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts hf.exe, huggingface-cli.exe and tiny-agents.exe are installed in 'C:\\Users\\geronimo.martinez-n\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\geronimo.martinez-n\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'C:\\Users\\geronimo.martinez-n\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\geronimo.martinez-n\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.5 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.0-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\geronimo.martinez-n\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached torch-2.9.0-cp311-cp311-win_amd64.whl (109.3 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "Successfully installed sympy-1.14.0 torch-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\geronimo.martinez-n\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\geronimo.martinez-n\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets huggingface_hub tokenizers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb70c2d",
   "metadata": {},
   "source": [
    "## 3. Conceptos esenciales\n",
    "- **Modelo**: pesos entrenados + arquitectura (`AutoModel*`, `AutoTokenizer`).\n",
    "- **Pipeline**: interfaz de alto nivel para tareas (clasificación, generación, QA, embeddings, etc.).\n",
    "- **Cache local**: HF guarda modelos y datasets en `~/.cache/huggingface/` para reutilización.\n",
    "- **Ejecución local vs remota**:\n",
    "  - *Local*: descargas pesos una vez, ejecutas con tu CPU/GPU.\n",
    "  - *Remota*: usas **Inference API** o **Spaces** (HF los ejecuta en su nube; puede haber límites o costos).\n",
    "- **Model Card**: ficha del modelo (uso previsto, limitaciones, licencias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de398b",
   "metadata": {},
   "source": [
    "## 4. Primeros pasos con `transformers` (local)\n",
    "Usaremos `pipeline` para hacer inferencia con modelos ligeros. La primera ejecución descarga los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1315d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geronimo.martinez-n\\Downloads\\hello_ai_vs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\geronimo.martinez-n\\Downloads\\hello_ai_vs\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\geronimo.martinez-n\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9524868726730347}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geronimo.martinez-n\\Downloads\\hello_ai_vs\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\geronimo.martinez-n\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Error while downloading from https://huggingface.co/gpt2/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hello AI classroom, today we will learn about virtual reality. These virtual reality classes will be taught by instructors who will understand their subject concepts by the use of VR to teach virtual reality in their classroom'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Clasificación de sentimientos (modelo ligero por defecto)\n",
    "clf = pipeline(\"sentiment-analysis\")\n",
    "print(clf(\"Este curso de IA en el aula me parece excelente.\"))\n",
    "\n",
    "# Generación de texto (modelo base pequeño)\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=30)\n",
    "print(gen(\"Hello AI classroom, today we will learn about\", num_return_sequences=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de99ec2",
   "metadata": {},
   "source": [
    "### Notas\n",
    "- La primera ejecución descargará los pesos (conexión requerida) y los guardará en caché.\n",
    "- En ejecuciones posteriores, se cargan desde disco.\n",
    "- Para usar GPU (si existe), pasa `device=0` al crear el pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba625054",
   "metadata": {},
   "source": [
    "## 5. Uso de la API de AutoModel/AutoTokenizer (más control)\n",
    "Cuando necesites más control que `pipeline`, usa las clases automáticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79715852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geronimo.martinez-n\\Downloads\\hello_ai_vs\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\geronimo.martinez-n\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001B813E27390>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f4e6305b-2aff-4016-ada4-24a4f5a9c93d)')' thrown while requesting GET https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEGATIVE': 0.0004911534488201141, 'POSITIVE': 0.9995088577270508}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "inputs = tok(\"I love practical AI courses.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "pred = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "print({\"NEGATIVE\": pred[0], \"POSITIVE\": pred[1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bff309",
   "metadata": {},
   "source": [
    "### Explicación detallada: Clasificación de sentimientos con Hugging Face\n",
    "\n",
    "Este ejemplo muestra cómo interactuar directamente con un modelo de Hugging Face sin usar `pipeline`, para comprender el flujo interno de tokenización, inferencia y decodificación.\n",
    "\n",
    "---\n",
    "\n",
    "### Importaciones\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "```\n",
    "\n",
    "- `AutoTokenizer`: descarga el **tokenizador** adecuado al modelo (convierte texto a IDs numéricos).  \n",
    "- `AutoModelForSequenceClassification`: carga el **modelo neuronal** y sus **pesos** preentrenados para tareas de **clasificación** (por ejemplo, sentimiento).  \n",
    "- `torch`: motor numérico (PyTorch) que gestiona tensores y operaciones de la red neuronal.\n",
    "\n",
    "---\n",
    "\n",
    "### Seleccionar el modelo\n",
    "\n",
    "```python\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "```\n",
    "\n",
    "- `distilbert`: versión compacta de **BERT** (Distilled BERT).  \n",
    "- `base-uncased`: vocabulario sin distinción de mayúsculas/minúsculas.  \n",
    "- `finetuned-sst-2`: ajustado sobre el dataset **Stanford Sentiment Treebank v2**, especializado en **análisis de sentimiento (positivo/negativo)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Cargar el tokenizador y el modelo\n",
    "\n",
    "```python\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "```\n",
    "\n",
    "- `from_pretrained()` busca el modelo en la caché local o lo descarga del **Hugging Face Hub**.  \n",
    "- El paquete incluye pesos, configuración, tokenizador y metadatos.  \n",
    "- `tok` traduce texto → números; `model` ejecuta la red neuronal con esos números.\n",
    "\n",
    "---\n",
    "\n",
    "### Tokenizar el texto\n",
    "\n",
    "```python\n",
    "inputs = tok(\"I love practical AI courses.\", return_tensors=\"pt\")\n",
    "```\n",
    "\n",
    "El tokenizador convierte la frase en IDs y máscaras de atención:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'input_ids': tensor([[101, 1045, 2293, 3331, 9935, 4822, 1012, 102]]),\n",
    "    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "}\n",
    "```\n",
    "\n",
    "- `input_ids`: IDs numéricos correspondientes a las subpalabras.  \n",
    "- `attention_mask`: marca con 1 las posiciones activas y con 0 el relleno (padding).  \n",
    "- `return_tensors=\"pt\"` devuelve tensores de PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### Inferencia (paso hacia adelante del modelo)\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "```\n",
    "\n",
    "- `torch.no_grad()`: desactiva el cálculo de gradientes (modo inferencia, más eficiente).  \n",
    "- `model(**inputs)`: pasa los tensores por las capas de la red neuronal.  \n",
    "- `.logits`: salida sin normalizar, vector con una puntuación por clase.  \n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "tensor([[-2.13, 3.56]])\n",
    "```\n",
    "→ Puntuación baja para *NEGATIVE*, alta para *POSITIVE*.\n",
    "\n",
    "---\n",
    "\n",
    "### Convertir logits en probabilidades\n",
    "\n",
    "```python\n",
    "pred = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "```\n",
    "\n",
    "- `softmax`: convierte las puntuaciones en probabilidades que suman 1.  \n",
    "- `dim=-1`: aplica la operación sobre la última dimensión (las clases).  \n",
    "- `tolist()[0]`: transforma el tensor a una lista Python.\n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "[0.01, 0.99]\n",
    "```\n",
    "→ 1% negativo, 99% positivo.\n",
    "\n",
    "---\n",
    "\n",
    "### Mostrar el resultado\n",
    "\n",
    "```python\n",
    "print({\"NEGATIVE\": pred[0], \"POSITIVE\": pred[1]})\n",
    "```\n",
    "Salida:\n",
    "```python\n",
    "{'NEGATIVE': 0.0123, 'POSITIVE': 0.9877}\n",
    "```\n",
    "El modelo predice un sentimiento **claramente positivo** para la frase.\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen del flujo interno\n",
    "\n",
    "| Etapa | Qué hace | Tipo de dato |\n",
    "|--------|-----------|--------------|\n",
    "| Tokenizer | Convierte texto → IDs | Diccionario de tensores |\n",
    "| Modelo | Procesa los tensores y produce logits | Tensor 2D |\n",
    "| Softmax | Convierte logits → probabilidades | Lista o array |\n",
    "| Salida final | Devuelve un diccionario con clases y probabilidades | Dict |\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptos clave para el aula\n",
    "\n",
    "- **Tokenización:** convierte lenguaje humano a números comprensibles por el modelo.  \n",
    "- **Logits vs Probabilidades:** logits son puntuaciones sin escalar; `softmax` las convierte en probabilidades.  \n",
    "- **Fine-tuning:** ajuste de un modelo base a una tarea específica.  \n",
    "- **Inferencia:** uso del modelo para predecir (sin entrenamiento).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5905a99",
   "metadata": {},
   "source": [
    "## 6. Trabajar con `datasets`\n",
    "La librería `datasets` permite cargar datos públicos del Hub y tratarlos como DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b78ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geronimo.martinez-n\\Downloads\\hello_ai_vs\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\geronimo.martinez-n\\.cache\\huggingface\\hub\\datasets--ag_news. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 602993.76 examples/s]\n",
      "Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 542831.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Carga un dataset de ejemplo (pequeño)\n",
    "ds = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(ds)\n",
    "print(ds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b2ff0",
   "metadata": {},
   "source": [
    "## 7. Hugging Face Inference API (remoto)\n",
    "Si no quieres descargar ni ejecutar modelos localmente, puedes llamar a la **Inference API**. Requiere una cuenta HF y, en algunos casos, **token de acceso** (con cuota gratuita limitada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fd6075",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'huggingface_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceClient\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Opcional: configura tu token HF si es necesario\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# os.environ['HF_TOKEN'] = 'hf_...'\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'huggingface_hub'"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "# Opcional: configura tu token HF si es necesario\n",
    "# os.environ['HF_TOKEN'] = 'hf_...'\n",
    "\n",
    "client = InferenceClient(model=\"gpt2\")  # modelo público sencillo\n",
    "out = client.text_generation(\"Hello from remote HF Inference API!\", max_new_tokens=32)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88e0a0",
   "metadata": {},
   "source": [
    "### ¿Cuándo usar local vs remoto?\n",
    "- **Local**: control total, sin costos por uso; requiere recursos (CPU/GPU/RAM) y descarga de pesos.\n",
    "- **Remoto (Inference API/Spaces)**: cero instalación, útil para demos/producción; puede tener límites/costos.\n",
    "- En docencia: comenzar local con modelos pequeños; escalar a remoto si es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bfd94",
   "metadata": {},
   "source": [
    "## 8. Embeddings (representaciones vectoriales)\n",
    "Los embeddings son útiles en **búsqueda semántica**, **RAG** y **análisis de similitud**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "emb_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "emb_tok = AutoTokenizer.from_pretrained(emb_model_id)\n",
    "emb_model = AutoModel.from_pretrained(emb_model_id)\n",
    "\n",
    "def embed(texts):\n",
    "    # Tokenización con padding/truncado\n",
    "    batch = emb_tok(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = emb_model(**batch)\n",
    "    # Mean pooling simple sobre la última capa\n",
    "    tokens = out.last_hidden_state  # [batch, seq, hidden]\n",
    "    mask = batch[\"attention_mask\"].unsqueeze(-1)  # [batch, seq, 1]\n",
    "    masked = tokens * mask\n",
    "    sent_emb = masked.sum(dim=1) / mask.sum(dim=1)\n",
    "    return F.normalize(sent_emb, p=2, dim=1)\n",
    "\n",
    "e = embed([\"inteligencia artificial en educación\", \"clase de programación\", \"oxigenación en hidroeléctricas\"])\n",
    "print(e.shape)\n",
    "# Similitud coseno entre primera y segunda\n",
    "sim = (e[0] @ e[1]).item()\n",
    "print(\"Cosine similarity (0 vs 1):\", round(sim, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06be84",
   "metadata": {},
   "source": [
    "# Comprendiendo los Embeddings en Inteligencia Artificial\n",
    "\n",
    "## ¿Qué es un *embedding*?\n",
    "\n",
    "Un **embedding** es una **representación numérica** de un dato (texto, imagen, audio, etc.) en un **espacio vectorial continuo**.  \n",
    "En lugar de trabajar directamente con palabras o símbolos, los modelos de IA los transforman en **vectores de números reales**, de modo que conceptos similares queden **cercanos entre sí** en ese espacio.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| Palabra | Embedding (simplificado) |\n",
    "|----------|--------------------------|\n",
    "| \"perro\" | [0.82, 0.10, 0.44, …] |\n",
    "| \"gato\"  | [0.80, 0.12, 0.46, …] |\n",
    "| \"avión\" | [-0.30, 0.90, -0.12, …] |\n",
    "\n",
    "Si calculamos la distancia entre vectores, veremos que **“perro”** y **“gato”** están mucho más cerca que **“perro”** y **“avión”**.  \n",
    "Esto significa que el modelo **captura relaciones semánticas** (de significado) entre palabras.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuición geométrica\n",
    "\n",
    "Los embeddings convierten el lenguaje en **puntos en un espacio N-dimensional**, donde la geometría refleja las relaciones conceptuales:\n",
    "\n",
    "- Distancias pequeñas → conceptos similares.  \n",
    "- Distancias grandes → conceptos distintos.  \n",
    "- A veces incluso se pueden representar relaciones lineales:\n",
    "  ```\n",
    "  vector(\"rey\") - vector(\"hombre\") + vector(\"mujer\") ≈ vector(\"reina\")\n",
    "  ```\n",
    "\n",
    "Por eso los embeddings son la base de muchas tareas de **razonamiento semántico** en IA.\n",
    "\n",
    "---\n",
    "\n",
    "## Cómo se generan los embeddings\n",
    "\n",
    "Los embeddings se **aprenden** durante el entrenamiento de modelos.  \n",
    "En redes neuronales, la primera capa suele ser una **capa de embedding** que asigna a cada palabra un vector.\n",
    "\n",
    "- En modelos clásicos como **Word2Vec**, **GloVe** o **FastText**, los embeddings se entrenan explícitamente observando qué palabras aparecen juntas.\n",
    "- En modelos modernos (**BERT**, **GPT**, **DistilBERT**), los embeddings son el resultado de las **capas internas** de atención.  \n",
    "  Se pueden extraer desde cualquiera de esas capas usando librerías como `transformers`.\n",
    "\n",
    "---\n",
    "\n",
    "## Embeddings en Hugging Face\n",
    "\n",
    "Podemos usar un modelo especializado en *sentence embeddings* (como los de **Sentence Transformers**) para convertir textos completos en vectores comparables:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "def embed(texts):\n",
    "    batch = tok(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "    # Promedio (mean pooling)\n",
    "    tokens = out.last_hidden_state\n",
    "    mask = batch[\"attention_mask\"].unsqueeze(-1)\n",
    "    sent_emb = (tokens * mask).sum(1) / mask.sum(1)\n",
    "    return F.normalize(sent_emb, p=2, dim=1)\n",
    "\n",
    "emb = embed([\"inteligencia artificial\", \"aprendizaje automático\", \"física cuántica\"])\n",
    "print(emb.shape)\n",
    "```\n",
    "\n",
    "Cada texto se convierte en un vector de dimensión 384 o 768 (según el modelo).\n",
    "\n",
    "---\n",
    "\n",
    "## Comparación entre embeddings\n",
    "\n",
    "Para comparar embeddings se usa la **similitud del coseno**, que mide el ángulo entre los vectores:\n",
    "\n",
    "\\[\n",
    "\\text{similaridad}(A,B) = \\frac{A \\cdot B}{||A|| \\, ||B||}\n",
    "\\]\n",
    "\n",
    "Valores:\n",
    "- 1.0 → textos muy similares.  \n",
    "- 0.0 → no relacionados.  \n",
    "- -1.0 → opuestos conceptualmente.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "sim = float((emb[0] @ emb[1]).item())\n",
    "print(\"Similitud coseno:\", round(sim, 4))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Aplicaciones educativas y prácticas\n",
    "\n",
    "Los embeddings son fundamentales en muchos sistemas modernos:\n",
    "\n",
    "| Aplicación | Uso de embeddings |\n",
    "|-------------|------------------|\n",
    "| **RAG (Retrieval-Augmented Generation)** | Recuperar documentos relevantes antes de generar texto. |\n",
    "| **Búsqueda semántica** | Encontrar textos “parecidos” por significado, no por palabras exactas. |\n",
    "| **Clasificación y clustering** | Agrupar frases o estudiantes por temas o patrones. |\n",
    "| **Recomendadores** | Calcular similitud entre recursos educativos. |\n",
    "| **Análisis de discurso** | Detectar similitudes en respuestas escritas o redacciones. |\n",
    "\n",
    "---\n",
    "\n",
    "## Buenas prácticas\n",
    "\n",
    "- Normaliza los vectores antes de compararlos (`F.normalize` o división por norma).  \n",
    "- Usa embeddings de **oraciones** (Sentence Transformers) para comparar frases completas.  \n",
    "- Almacena embeddings en bases vectoriales como **FAISS** o **Chroma** para búsquedas rápidas.  \n",
    "- Recuerda que los embeddings reflejan el **sesgo** del modelo con el que se entrenaron; úsalo de forma crítica y responsable.\n",
    "\n",
    "---\n",
    "\n",
    "## En resumen\n",
    "\n",
    "> Un *embedding* es una forma matemática de representar significado.  \n",
    "> Convierte información simbólica (palabras, imágenes, sonidos) en **vectores** donde la distancia representa **relación semántica**.  \n",
    "> Son el puente entre el lenguaje humano y el razonamiento numérico de los modelos de IA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2d8e1",
   "metadata": {},
   "source": [
    "## 9. Tokenización con `tokenizers`\n",
    "`tokenizers` (Rust + Python) es la base de la tokenización rápida y reproducible para modelos modernos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "# Ejemplo mínimo: crear un tokenizer vacío BPE (demostrativo)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "# Nota: entrenar un tokenizer real requiere un corpus y procesos adicionales (no cubierto aquí).\n",
    "print(\"Tokenizer BPE de ejemplo creado (demo).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826326e4",
   "metadata": {},
   "source": [
    "## 10. Buenas prácticas, licencias y ética\n",
    "- Revisa la **Model Card** antes de usar un modelo: usos previstos, sesgos, limitaciones.\n",
    "- Respeta las **licencias** de modelos/datasets; algunos son comerciales, otros sólo para investigación.\n",
    "- En el aula: evita datos personales reales, valida salidas y cita fuentes de modelo/dataset.\n",
    "- Optimiza recursos: usa modelos pequeños para clases; documenta versiones y hashes de commit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0a344",
   "metadata": {},
   "source": [
    "## 11. Solución de problemas comunes\n",
    "- **`ModuleNotFoundError`**: reinstala el paquete faltante; reinicia kernel.\n",
    "- **`OSError: Can't load tokenizer/model`**: el ID del modelo es incorrecto o no tienes permisos.\n",
    "- **Falta de RAM/GPU**: usa modelos más pequeños o recurre a Inference API.\n",
    "- **Rate limit (remoto)**: espera o considera un plan de pago.\n",
    "- **Conectividad**: comprueba proxies/firewalls y credenciales (HF_TOKEN si aplica)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219454f3",
   "metadata": {},
   "source": [
    "## 12. Próximos pasos (sugerencias)\n",
    "- Fine-tuning ligero con **PEFT/LoRA** sobre un dataset pequeño.\n",
    "- Construir un mini-**RAG** con embeddings + FAISS/Chroma.\n",
    "- Publicar una demo en **HF Spaces** (Gradio) para compartir con estudiantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
